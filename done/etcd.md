# 《云原生分布式存储基石：ETCD深入解析》笔记

高可用分布式键值数据库，机器间通过raft算法通信，具有强一致性

- [《云原生分布式存储基石：ETCD深入解析》笔记](#云原生分布式存储基石etcd深入解析笔记)
  - [分布式系统与一致性协议](#分布式系统与一致性协议)
    - [一致性](#一致性)
      - [以数据为中心的一致性模型](#以数据为中心的一致性模型)
      - [以用户为中心的一致性模型](#以用户为中心的一致性模型)
      - [复制状态机](#复制状态机)
    - [Raft协议](#raft协议)
    - [日志压缩及快照](#日志压缩及快照)
  - [为什么使用ETCD](#为什么使用etcd)
    - [etcd简介](#etcd简介)
    - [使用场景](#使用场景)
      - [键值对存储](#键值对存储)
      - [服务注册与发现](#服务注册与发现)
      - [消息发布与订阅](#消息发布与订阅)
      - [负载均衡](#负载均衡)
      - [分布式通知与协调](#分布式通知与协调)
      - [分布式锁](#分布式锁)
      - [分布式队列](#分布式队列)
      - [集群监控与Leader竞选](#集群监控与leader竞选)
    - [概念词汇表](#概念词汇表)
  - [使用](#使用)
    - [单机部署](#单机部署)
    - [多节点集群化部署](#多节点集群化部署)
      - [静态配置](#静态配置)
      - [服务发现](#服务发现)
    - [常用命令](#常用命令)
  - [etcd API](#etcd-api)
    - [V2](#v2)
    - [v3](#v3)
  - [运维与稳定性](#运维与稳定性)
    - [升级](#升级)
    - [运行时重配置](#运行时重配置)
    - [参数调优](#参数调优)
    - [维护](#维护)
    - [灾难恢复](#灾难恢复)
    - [网关](#网关)
    - [grpc代理](#grpc代理)
  - [安全](#安全)
    - [访问安全](#访问安全)
    - [传输安全](#传输安全)
  - [高级篇](#高级篇)

## 分布式系统与一致性协议

分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统

分布式系统的设计目标：可用性，可扩展性，容错性，性能。

### 一致性

通俗地讲，一致性就是不同的副本服务器认可同一份数据。很多场景都要求一致性，但并不是所有的系统都要求是强一致的。甚至银行转账之类的场景，也存在不一致的时间窗口。

从客户端来看，一致性主要是指多并发访问时如何获取更新过的数据的问题。从服务端来看，则是更新如何复制分布到整个系统。

#### 以数据为中心的一致性模型

严格一致性：任一次读都能读到某个数据最近一次写的数据、系统中所有进程看到的操作顺序都和全局时钟下的顺序一致。强一致的子操作组成的父操作也是强一致的。

顺序一致性：也成为可序列化，放弃全局时钟的约束，改为分布式逻辑时钟实现。所有的进程都以相同的顺序看到所有的修改，实际上就是要求所有进程达成一致。弱于严格一致性。

因果一致性：

> 本地顺序：本进程中事件执行的顺序为本地因果顺序。
>
> 异地顺序：如果读操作返回写操作的值，写操作应在读操作之前。
>
> 闭包传递：与时钟向量里定义的一样，如果a->b且b->c，则有a->c
>
> 否则，操作之间为并发关系。对于有潜在因果关系的写操作，所有进程执行顺序相同。并发写操作在不同主机上看到的顺序可以不同。一般弱于顺序一致性。

可串行化一致性：可序列化允许对操作重新进行任意排序，只要顺序看起来是原子的即可，一致性可能很强又可能很弱。

#### 以用户为中心的一致性模型

最终一致性：如果更新的时间间隔比较长，所有副本能够最终达到一致性。

#### 复制状态机

一个分布式的复制状态机系统由多个复制单元组成，每个复制单元均是一个状态机，它的状态保存在一组状态变量中。状态机的状态能够并且只能通过外部命令来改变。

一组状态变量”通常是基于操作日志来实现的，这时一致性算法的主要工作就变成了如何保证操作日志的一致性。但是，指令在状态机上的执行顺序并不一定等同于指令的发出顺序或接收顺序。

拜占庭错误：最强的错误，某些恶意节点扰乱系统正常运行，包括选择性不传递、选择性伪造消息。进程失败错误：最弱的错误，出错时节点停止运行，其他所有的节点都知道该节点发生了错误。要容忍N个拜占庭错误，至少要有2N+1个复制节点，如果把错误类型缩小到进程失败，则至少需要N+1个。

FLP定理：在允许节点失效的场景下，基于异步通信方式的分布式协议无法确保在有限的时间内达成一致性，即使只有一个节点失效。

### Raft协议

Raft协议把问题分解成领袖选举、日志复制、安全性、成员关系变化这几个子问题。在一个领袖节点发生故障后必须重新给出一个新的领袖节点；领袖节点从客户端接收操作请求，然后将操作日志复制到集群中的其他服务器上，并强制要求其他服务器的日志和自己保持一致；安全性指的是如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目；配置发生变化时集群能继续工作。

Raft集群中的节点通过远端过程调用（RPC）来进行通信，Raft算法的基本操作只需2种RPC即可完成。RequestVote RPC是在选举过程中通过旧的Leader触发的，AppendEntries RPC是领导人触发的，目的是向其他节点复制日志条目和发送心跳。

基于简化操作和效率等因素考虑，Raft算法采用的是非对称节点关系模型。只有主节点有决策权，任何时刻只有一个主节点，客户端只和主节点交互。一共有3类角色，Leader、Candidate、Follower。

一开始所有节点都是群众，第一轮选举都可以成为候选人，一旦某位候选人得到半数选票，就成为领袖，开始一个新的任期，其他候选人回到群众身份并接受领导。任期实际上承担了逻辑时钟的作用，同时检测过期信息。每个节点在本地维护一个任期值，节点通信时交换当前任期号，如果一个节点的任期号比其他的小，则将本地的任期号更换为较大的任期号；如果一个候选人或领袖意识到他的任期号比别的小，则切换回群众；如果一个节点收到请求携带过期的任期号，则拒绝响应。

每个Raft节点都有一个选举定时器，所有的Raft节点最开始以Follower角色运行时，都会启动这个选举定时器。不过，每个节点的选举定时器时长均不相等。Leader在任期内必须定期向其他节点广播心跳，Follower每次收到心跳就将自己的选举定时器清零重置，如果选举定时器超时，Follower就假定Leader不存在并发起选举。如果一个Foller决定参加选举，首先将本地维护的任期号+1，将自己的状态切换到Candidate，并为自己投票，向其他节点发送RequestVote RPC，要求投票给自己。如果得到了大多数选票则成为Leader，一个节点只能为一个Candidate投票，先到先得的投票给最早拉票的候选者；如果等待投票时收到了声称自己是Leader的心跳包，会检查任期号，如果比自己的任期号大则认为合法，否则拒绝并返回新的任期号告诉其已经过期；如果没有Candidate获得半数以上选票，为每个候选人设置超时时间，超时后候选人自增任期号并发起新的拉票，通过随机超时的时间以使得大多数时候只有一个节点率先超时。

一旦某个领导人赢得选举，就开始接收客户端请求，每个请求解析成一条需要复制状态机执行的指令，Leader将指令作为一条新的日志条目加入它的日志文件，并行地向其他Raft节点发起AppenEntries RPC，要求其他节点复制这个日志，当日志被“安全”地复制之后，Leader将日志应用到状态机中并向客户端返回执行结果。如果Follower没有及时响应，Leader会无限重试（甚至在它响应了客户端之后）直到所有Follower最终存储了一样的日志条目。每条日志包括索引、任期号、指令。如果某个条目被复制到半数以上的节点，该条目就是可提交的。Raft算法保证可被提交的日志条目是持久化的，并且最终是会被所有状态机执行的。

领导人跟踪所知道的被提交日志条目的最大索引值，并且这个值包含在向其他节点发送到AppendEntries RPC中，一旦Follower得知某个日志条目被提交，它会将其应用至本地的状态机。日志机制：**如果不同的日志中的两个条目有相同的索引号和任期号，它们存储的命令相同，且他们之前的条目完全一样。**这由两个条件保证，领导人在一个任期里在给定的一个日志索引位置上最多创建一条日志条目，同时该条目在日志文件中的槽位永远也不会改变；领导人发送消息试图追加条目时，也把新日志条目前一个槽位的日志条目的任期号和索引号包含在消息体，如果Follower没在自己的日志中找到相同任期号和索引的日志，就拒绝该RPC。

一个新的Leader被选举出来，日志与Follower可能不一样，Follower可能丢失或者多出一些未提交的条目。Leader会强制Follower复制它的日志，找到第一个不一致的位置然后删除之后的所有日志，并把自己的日志发送给Follower。

一次正常的复制流程：客户端向Leader发送写请求；Leader将请求解析成操作指令追加到日志文件；为每个follower广播AppendEntries RPC；Follower通过一致性检查，选择从哪个位置开始追加日志条目；一旦日志项提交成功，Leader就应用该条目对应的指令并向客户端返回操作结果；Leader通过AppendEntries RPC将已经在大多数节点上成功提交的日志告知Follower；Follower收到后应用至本地状态机。

**没有包含所有已提交日志条目的节点成为不了领导人；日志条目只能从Leader流向Follower。领导人永远不会覆盖已经存在的日志条目。**如果一个节点的日志比候选人的日志更新，就会拒绝候选人的投票请求，比较的依据是日志文件中最后一个条目的索引和任期号。Raft算法对日志提交条件增加了一个额外的限制：要求Leader在当前任期至少有一条日志被提交，即被超过半数的节点写盘。

选举并且保持一个稳定的领导人存在的条件：节点发送RPC并收到响应的平均时间<<选举超时时间<<单个节点故障平均间隔时间。

领导人故障的情况：（1）数据到达之前，不会影响一致性；（2）数据到达Leader但未复制到Follower，此时处于未提交状态，客户端不会收到ack而是认为超时发起重试；（3）到达Leader并成功复制到部分节点，但还未向Leader响应接收时，数据在Follower处于未提交状态，只有拥有新数据的节点能被选为Leader；（4）数据到达Leader节点，成功复制到Follower的所有节点上，但还未向Leader响应接收，也能保持一致，但由于客户端不知道有没有提交成功，可重试提交，因此要求rpc是幂等的；（5）数据到达Leader，成功复制到所有或者大多数节点，Leader已提交但Follower未提交，与阶段3一致；（6）所有节点都已提交，但还未响应Client，此时内部已经是一致的；（7）网络分区导致双Leader，向旧的Leader提交不可能成功，向新的Leader提交可以成功，网络恢复后旧的Leader自动降级Follow并从新的Leader同步。

### 日志压缩及快照

Raft快照文件的特点：每个节点独立创建、只包含被提交的日志条目；存储节点某一时刻复制状态机的状态；全量式而非增量；在快照中存储少量元数据，比如被快照取代的最后一个日志条目的索引位置和任期号（为了支持一致性检查）。

Follower可以在没有领导人的情况下生成快照，这违背了强领导人原则，但是可接受的，因为快照创建时一致性已经达成了。

快照系统会对系统的悉能造成一定的影响，太频繁会消耗I/O带宽和CPU资源，太久则有耗尽存储空间的风险。

## 为什么使用ETCD

分布式系统的困难主要体现在“部分失败”上，在信息传送过程中网络出现了故障，发送者不知道接收者是否收到了消息。

etcd是一个键值存储仓库，用于配置共享和服务发现。常见的etcd使用场景包括：服务发现、分布式锁、分布式数据队列、分布式通知和协调 、主备选举等。

Consul的优势在于服务发现，etcd的优势在于配置信息共享和方便运维，ZooKeeper的优势在于稳定性。

### etcd简介

etcd支持从非Leader读取数据以提高性能；具有一定的容错能力；默认数据一更新就落盘持久化。

一个用户请求发送过来通过网络层转发给存储模块进行具体的事务处理，如果涉及到节点状态更新，则交给Raft模块进行仲裁和日志记录，再同步给其他的节点，只有半数节点确认了该节点状态的修改之后才会进行数据持久化。

在启动之初就会创建并维持节点间的连接，节点间通过网络传递数据。

消息的大小不尽相同，快照可能比较大，心跳则很小。因为抽象出了两种消息通道，stream和pipeline，一个是点对点的双向传输用来处理数据量小的消息；另一个可以开启短链接传输数据，用来处理数据量大的消息，同时发送N个消息，用完即关闭。分开处理可以避免阻塞心跳包的传输，只有在stream不可用时，才会用pipeline发送数据量小的消息。

### 使用场景

#### 键值对存储

#### 服务注册与发现

- 强一致性、高可用的服务存储目录（通过raft算法支持）
- 注册服务和服务健康状况的机制。 用户可以在 etcd 中注册服务，并且对注册的服务配置 key TTL，定时保持服务的心跳以达到监控健康状态的效果
- 查找和连接服务的机制。通过在 etcd 指定的主题下注册的服务也能在对应的主题下查找到。可以在每个服务机器上都部署一个 Proxy 模式的 etcd，这样就可以确保访问 etcd 集群的服务都能够互相连接

#### 消息发布与订阅

- etcd集中管理配置信息。应用在启动的时候主动从etcd获取一次配置信息，同时在etcd节点上注册一个Watcher并等待，以后每次配置有更新的时候，etcd都会实时通知订阅者
- 分布式搜索服务中，索引的元信息和服务器集群机器的节点状态存放在etcd中，供各个客户端订阅使用。使用etcd的key TTL功能可以确保机器状态是实时更新的。
- 分布式日志收集系统。收集器通常是按照应用（或主题）来分配收集任务单元，因此可以在etcd上创建一个以应用（主题）命名的目录P，并将这个应用（主题相关）的所有机器ip，以子目录的形式存储到目录P上，然后设置一个 etcd 递归的Watcher，递归式的监控应用（主题）目录下所有信息的变动。这样就实现了机器IP（消息）变动的时候，能够实时通知到收集器调整任务分配。
- 动态自动获取信息与人工干预修改信息请求内容。通常是暴露出接口，例如JMX接口，来获取一些运行时的信息。引入etcd之后，就不用自己实现一套方案了，只要将这些信息存放到指定的etcd目录中即可，etcd的这些目录就可以通过HTTP的接口在外部访问。

#### 负载均衡

- etcd本身分布式架构存储的信息支持负载均衡
- 利用etcd维护一个负载均衡节点表

#### 分布式通知与协调

- 类似消息发布和订阅，用到了etcd中的Watcher机制，不同系统都在etcd上对同一个目录进行注册，同时设置Watcher观测该目录的变化（如果对子目录的变化也有需要，可以设置递归模式），当某个系统更新了etcd的目录，那么设置了Watcher的系统就会收到通知，并作出相应处理。
- 低耦合的心跳检测
- 系统调度。管理人员修改etcd上某些目录节点的状态，而etcd就把这些变化通知给注册了Watcher的客户端，完成调度
- 工作汇报。子任务启动后，到etcd来注册一个临时工作目录，并且定时将自己的进度进行汇报（将进度写入到这个临时目录）

#### 分布式锁

- 分布式环境下不仅需要保证进程可见，还需要考虑进程与锁之间的网络问题
- 锁服务有两种使用方式，一是保持独占（所有获取锁的用户最终只有一个可以得到），二是控制时序（所有想要获得锁的用户都会被安排执行，但是获得锁的顺序也是全局唯一的，同时决定了执行顺序）

#### 分布式队列

- 创建一个先进先出的队列保证顺序
- 在队列目录中另外创建一个condition，达到条件时统一按顺序执行，条件可以是队列的大小/某个任务不在队列中/执行任务的通知。

#### 集群监控与Leader竞选

- 通过watcher + ttl key来监控节点状态。
- 使用分布式锁完成Leader竞选。

### 概念词汇表

- Raft：etcd所采用的保证分布式系统强一致性的算法
- Node：一个Raft状态机实例
- Member： 一个etcd实例。它管理着一个Node，并且可以为客户端请求提供服务
- Cluster：由多个Member构成可以协同工作的etcd集群
- Peer：对同一个etcd集群中另外一个Member的称呼
- Client： 向etcd集群发送HTTP请求的客户端
- WAL：预写式日志，etcd用于持久化存储的日志格式
- snapshot：etcd防止WAL文件过多而设置的快照，存储etcd数据状态
- Proxy：etcd的一种模式，为etcd集群提供反向代理服务
- Leader：Raft算法中通过竞选而产生的处理所有数据提交的节点
- Follower：竞选失败的节点作为Raft中的从属节点，为算法提供强一致性保证
- Candidate：当Follower超过一定时间接收不到Leader的心跳时转变为Candidate开始竞选
- Term：某个节点成为Leader到下一次竞选时间，称为一个Term
- Index：数据项编号。Raft中通过Term和Index来定位数据

## 使用

### 单机部署

### 多节点集群化部署

#### 静态配置

适用于线下环境，各member需要互相感知，要求节点个数和地址已知。

#### 服务发现

简单来说就是使用一个现有的etcd集群启动一个新的etcd集群。有etcd自发性和dns自发现两种模式。

### 常用命令

写入一个key
> etcdctl put foo bar

读取一个key
> etcdctl get foo
> etcdctl get foo foo3 # 获取一个范围内的key，左闭右开
> etcdctl get --prefix --limit=2 foo # 获取指定前缀的key，加上limit限制输出结果数量
> etcdctl get --prefix --rev=2 foo # 读取某个版本的key
> etcdctl get --from-key b # 读取字典序大于b的所有key

删除一个key
> etcdctl del foo
> etcdctl del foo foo3 # 也支持prefix和from-key
> etcdctl del --prev-kv foo # 删除的同时返回对应的value

观察某个key，感知到变化
> etcdctl watch foo # 也支持范围或者prefix，通过-i可以watch多个key，通过rev可以观察某个版本开始的变化。

压缩某个版本号之前的空间
> etcdctl compact 5 # 再读取5之前的版本号会报错
> etcdctl get foo -w=json # 通过get一个key，不论存在与否都可以获取当前etcd客户端版本号

租约
> etcdctl lease grant 10
> etcdctl put foo1 barl --lease=1234abcd # 根据之前得到的租约号绑定一个租约，过期后删除上面绑定的所有key
> etcdctl lease revoke 1234abcd # 撤销租约，这会删除上面的所有key
> etcdctl lease keep-alive 1234abcd # 刷新TTL
> etcdctl lease timetolive --keys 1234abcd # 返回租约的TTL和剩余时间，加上--keys可以输出绑定的keys

## etcd API

### V2

一个etcd操作完成的标志是它已经通过一致性协议提交并已经被执行，即被存储引擎持久化存储了。v2主要提供读写API。

etcd所有的API都是原子的、顺序一致性的（除了watch可以保证线性一致性，watch需要检查结果的版本号来保证正确的顺序）。etcd保证可串行化的隔离，读操作不会看到任何中间数据。任何完成的操作都是持久的，所有可访问的数据也都是持久的。

当Follower和Leader的系统时钟相差1s以上时，etcd会发出警告，提示两者的时钟有较大的不同步，防止定义了TTL的key被过早或过晚的删除。

v2客户端只缓存1000条事件的历史记录，如果发生事件洪泛，就有可能导致事件丢失。

一般情况下在处理watch返回的信息时，最好在单独线程里面进行，这样不会阻塞watch。除了watch单个key以外，etcd还支持watch目录。

原子的CAS，先比较，如果不一样再交换；原子的CAD，先比较提供条件是否与当前条件相等，相等则删除对应的key。

etcd会统计集群运行时的一些数据，例如请求时延、数据带宽、运行时长等。这些数据通过API暴露给客户端。

### v3

通过grpc取代http和json，提高序列化和反序列化性能；一个客户端和服务端只需要建立一个tcp连接；租约机制同时管理绑定在租约上的多个key；对来自同一个客户端的watch的进行了多路复用；通过以历史记录为主索引的存储结构保存了key的所有历史变更记录，v3可以存储上十万条记录进行快速查询，用磁盘数据库代替了内存数据库；摒弃了v2的目录式层级化设计，将目录查询看成是相同前缀的key的查询；通过迷你事务可以定义一系列的条件语句，只有在还有条件满足时事务才执行成功，可以原子地比较并操作多个键值；支持增量快照和传输相对较大的快照，可以存储百万到千万级别的key；同一个用户的不同watcher只消耗一个go routine。

V3版本的API可分为键值、集群、维护、认证、观察和租约6类。

- etcd3的watch机制确保了监测到的事件具有有序、可靠与原子化的特点
  - 有序：事件按照revision排序，后发生的事件不会在前面的事件之前出现在watch流中
  - 可靠：某个事件序列不会遗漏其中任意的子序列
  - 原子性：事件列表确保包含完整的revision，在相同revision的多个key 上，更新不会分裂为几个事件列表

## 运维与稳定性

### 升级

etcd从2.3升级到3.0可以滚动升级，一个一个用3.0的etcd进程替换之前的。升级前要检查集群的健康状态；在隔离环境中测试依赖etcd的应用；备份etcd的数据目录。升级过程中，etcd支持集群内的节点存在混合版本，并使用最低版本的协议进行通信。集群内部的节点通过相互通信来决定集群的总体版本。

即使升级到v3, v2的数据存储依然能够通过v2 API对外使用，MVCC与旧的v2存储是隔离的，存储写数据不会影响另一个存储。

### 运行时重配置

etcd支持增量的运行时重配置，要求集群的大部分节点正常工作。通过两阶段配置更新来保证安全：通知集群新的配置和启动新集群。

### 参数调优

etcd的默认配置在本地网络环境通常能运行的很好，但网络延时很高时需要根据实际情况调整参数。

有两个重要的时间参数，一个是心跳间隔，即主节点通知从节点它还是领导者的频率，该参数应该设置成节点之间RTT（往返时延）的最大值。另一个是选举超时时间，即从节点等待多久没收到主节点的心跳就去尝试竞选领导者，一般至少是10倍RTT的时间以应对网络波动。

etcd对磁盘I/O的时延非常敏感，因为必须持久化他的日志。可以通过赋予etcd服务器更高的磁盘I/O权限来让他更稳定。

如果etcd主节点要处理大规模并发的客户端请求，可以提高etcd节点之间通信的网络带宽优先级。

### 维护

通过压缩历史版本丢弃给定版本之前的信息可以节省空间，但会造成存储碎片，可以通过defrag命令清除节点的存储碎片。

通过存储配额来保证集群操作的可靠性，如果某个节点的存储空间超过存储配额，会触发集群范围的告警，进入维护模式，只接受读和删除key。

### 灾难恢复

做好快照备份，发生问题后可以从一个已知的良好状态的时间点进行恢复，快照文件的完整性校验一般会在恢复时进行。恢复操作会初始化一个新的etcd节点，该etcd节点保留了之前的数据，并且配置将通过启动参数传入。

### 网关

etcd网关是一个简单的TCP代理，可用于向etcd集群转发网络数据。etcd网关支持多个后端etcd服务器，并且只支持轮询的负载均衡策略。

访问etcd的每个应用首先都要获取到etcd集群的广播客户端端点地址，通过网关可以让后端服务器的更新变得透明。但有性能要求和已有服务发现机制的情况下不推荐再使用网关。

### grpc代理

无状态的etcd反向代理，被设计成降低核心etcd集群的请求负载。对于横向扩展，可以合并watch以及为API请求绑定过期租约。此外还可以保护集群免受大流量冲击，缓存range request的结果。

grpc会尽可能的将新的watcher合并到已有的watcher中，在合并时，watcher的数据与etcd服务器上的数据可能会因为网络时延或缓存产生数据不－致的问题。使用代理时可能会从更早的版本号开始watch，当取消时，服务器的版本号可能会大于取消响应的版本号，这一般不会产生问题。

为保持租约可用，需要建立流用来定期发送心跳，grpc支持带租约流的合并。

假设一个应用期望对整个etcd数据空间具有完全的访问权限，但是该etcd集群是与其他应用共享的，可以用grpc代理隔离etcd的数据空间。经过代理的客户端请求的key被自动转换成带有用户定义的路径前缀，返回时自动去掉该前缀。

## 安全

### 访问安全

etcd认证体系分为User和Role，Role被授予给User，代表User拥有某项权利。root用户拥有对etcd访问的全部权限，并且必须在启动认证之前预先创建，root角色可以被授予任何用户。guest角色用于非认证使用。

etcd包括三种资源：权限资源（用户和角色信息）、键值资源、配置资源，只有root角色具有管理用户资源和配置资源的权限。默认guest角色拥有所有键值资源的完全访问权限，guest角色可以被root角色修改、撤销、甚至删除以减少未授权用户的能力。

etcd提供了两种权限：读和写。目前权限列表只支持ALLOW，不支持DENY。

当前，etcd只支持key的前缀和精确匹配，前缀字符以*结尾。

### 传输安全

etcd支持TLS协议通信加密，既能用于集群内部通信加密，也能加密客户端与服务端的通信。

## 高级篇

wait